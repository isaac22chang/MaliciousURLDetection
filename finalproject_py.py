# -*- coding: utf-8 -*-
"""FinalProject.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RAYD2JodvBNXrW62-7CojjuG20U8atYN

Hello all! In this google colab notebook, you will find the steps I took to analyze a malicious website Kaggle dataset using Bag Of Words and Tree Regression modeling.

Below are the modules I have imported including libraries and the dataset.
"""

import requests
import pandas as pd
import numpy as np
import sklearn
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
import 

from numpy import array
from numpy import argmax
import re

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder 
from sklearn.model_selection import cross_val_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_absolute_error

dataset = pd.read_csv("dataset.csv")
dataset.dropna(inplace=True)

"""The code cell below is about separating the dataset into string and integer features and switching the string features into integer features using Bag of Words."""

print(dataset.shape)
string_dataset = dataset.drop(['URL','URL_LENGTH', 'NUMBER_SPECIAL_CHARACTERS', 'CONTENT_LENGTH',
       'TCP_CONVERSATION_EXCHANGE', 'DIST_REMOTE_TCP_PORT', 'REMOTE_IPS',
       'APP_BYTES', 'SOURCE_APP_PACKETS', 'REMOTE_APP_PACKETS',
       'SOURCE_APP_BYTES', 'REMOTE_APP_BYTES', 'APP_PACKETS',
       'DNS_QUERY_TIMES', 'Type'],axis = "columns")

print(string_dataset.columns)


#It is implausible to one hot encode dates because of how unique they are
string_dataset = string_dataset.drop(['WHOIS_REGDATE', 'WHOIS_UPDATED_DATE'],axis="columns")

for i in string_dataset.columns:
  arr = ["hello"]
  firsttime = True
  # Looped through each feature in string_dataset and figured out all the unique datapoints 
  for j in string_dataset[i]:
    duplicated = False
    for k in arr: #alternate approach for when using an empty list: if j.lower() not in arr: **indent** arr.append(j.lower())
      if(j.lower() == k.lower()):
        duplicated = True
    if(duplicated == False):
      arr.append(j)
  arr.remove("hello")
  #loop through the unique features in the column list  and see if the dataset column has it or not
  for j in arr:
    encoded = [0]*(string_dataset.shape[0]) #or string.shape[0]
    b = 0
    for d in dataset[i]:
      if(d == j):
        encoded[b] = 1
      b = b + 1
    fencoded = array(encoded)
    dataset[j] = fencoded.tolist()
print(dataset.shape)

#Delete all the string columns since we have encoded them already
#If i wanted to use dates of server updates, you can use datetime() the librarys is import datetime
#result = date1-date2 
#use minmax scaler to help weigh the date data
revised_dataset = dataset.drop(['CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO', 'WHOIS_REGDATE',
       'WHOIS_UPDATED_DATE'],axis="columns")

"""Below, we have removed the URL section because each URL is unique and using Bag of Words would be useless because the machine learning model won't be able to make patterns.

The code cell below shows that there are 324 features, this lets us know that the bag of words model has successfully one hot encoded our string features by creating columns for each unique feature
"""

print(dataset.shape)
finished_dataset = revised_dataset.drop(['URL'],axis = "columns")

X = finished_dataset.drop('Type',axis = 'columns')
y = finished_dataset['Type']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

regressor = DecisionTreeClassifier(random_state = 0)
#prints out the columns to double check that the unique features are there
#for i in X.columns:
  #print(i)

#fitting the data to the default settings
regressor.fit(X_train,y_train)
y_predict = regressor.predict(X_test)
print(regressor.score(X_test,y_test))

"""Thanks for reading till here! Below is the last code block where I find if there are any better parameters that can fit the model. """

#Hyper parameter tuning

from scipy.stats import randint
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV


parameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}

tree =DecisionTreeClassifier()
tree_cv =GridSearchCV(tree,parameters,cv = 5)
tree_cv.fit(X_test,y_test)

print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))

#Shaply Values